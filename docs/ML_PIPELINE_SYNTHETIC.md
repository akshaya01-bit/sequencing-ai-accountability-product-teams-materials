# Synthetic ML pipeline for critical-turn classification

This document explains the synthetic ML pipeline implemented in:

- `code/ml/generate_synthetic_turns.py`
- `code/ml/train_critical_turn_classifier.py`

for the study:

> **Sequencing AI Recommendations and Accountability Practices in Product Teams**

---

## 1. Why critical turns?

The manuscript emphasizes **junior critical participation**—moments where junior
contributors challenge, question, or reframe AI suggestions and senior opinions.
These "critical turns" are central to understanding whether AI and accountability
practices are amplifying or suppressing junior voice.

In real deployments, labeling critical turns at scale would likely require a
combination of manual coding and ML support. This synthetic pipeline demonstrates
how such a classifier could be structured, without using any real conversational data.

---

## 2. Synthetic turn-level dataset

**File**

- `data/synthetic/study1_turns_labeled_synthetic.csv`

**Generated by**

- `code/ml/generate_synthetic_turns.py`

Each row represents a single **turn** (utterance) in a meeting, with fields:

- `team_id`, `meeting_id`, `agenda_item_id`
- `turn_id`, `turn_position`
- `speaker_role` (e.g., `junior_eng`, `junior_pm`, `senior_eng`, `director`)
- `is_junior` (0/1)
- `text` – a synthetic utterance string
- `is_critical` (0/1) – label for whether the turn is a "critical" contribution
- `ai_suggestion_visible` (0/1)
- `sequence_condition` ∈ {`AI_FIRST`, `HUMAN_FIRST`, `STATUS_QUO`}
- `accountability` ∈ {0,1}

The generator encodes a higher probability of critical turns when:

- The speaker is junior,
- Sequencing is HUMAN_FIRST, and
- Accountability is present,

mirroring the hypothesized conditions under which junior critical participation
is more likely.

---

## 3. Classifier training pipeline

**Script**

- `code/ml/train_critical_turn_classifier.py`

Pipeline steps:

1. Load the synthetic dataset and **filter to junior speakers** (`is_junior == 1`).
2. Extract:
   - Features: `text`
   - Labels: `is_critical` (0/1)
3. Split data into train/test sets (80/20, stratified on the label).
4. Represent text using TF–IDF with unigrams and bigrams.
5. Train a logistic regression classifier with class weights balanced.
6. Evaluate on the test set:
   - Accuracy
   - Precision / recall / F1 for the critical class
   - Confusion matrix

**Outputs**

- `models/critical_turn_classifier_synthetic.joblib`  
  – Serialized model (TF–IDF vectorizer + logistic regression classifier).

- `fig/critical_turn_classifier_metrics.csv`  
  – Summary metrics (accuracy, precision, recall, F1, test set size).

- `fig/critical_turn_classifier_metrics.txt`  
  – Human-readable report including a classification report and confusion matrix.

---

## 4. Relation to the manuscript

In the full study, a similar pipeline could be used to:

- Assist human annotators in identifying candidate critical turns.
- Support semi-automated coding of large volumes of meeting transcripts.
- Provide robustness checks (e.g., re-running analyses using ML-assisted labels).

The synthetic implementation here ensures that:

- The structure of the ML pipeline is visible and inspectable.
- No real conversational data is exposed.
- The roles of junior vs. senior speakers and the experimental conditions
  remain explicit in the feature space (e.g., via `speaker_role` and metadata).

---

## 5. Running the ML pipeline

From the repository root:

```bash
python3 -m pip install scikit-learn joblib pandas

# Generate synthetic turn-level data
python3 code/ml/generate_synthetic_turns.py

# Train and evaluate the classifier
python3 code/ml/train_critical_turn_classifier.py
